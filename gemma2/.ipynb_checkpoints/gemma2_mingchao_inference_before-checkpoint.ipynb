{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b068940d-5c10-4a1b-83ce-46c271bdf829",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: HF_HOME=/root/autodl-tmp/hf\n",
      "env: HF_HUB_CACHE=/root/autodl-tmp/hf\n"
     ]
    }
   ],
   "source": [
    "# %env HF_ENDPOINT=https://hf-mirror.com\n",
    "%env HF_HOME=/root/autodl-tmp/hf\n",
    "%env HF_HUB_CACHE=/root/autodl-tmp/hf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "32f5270a-65dd-46d1-96f9-b290bc606124",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import subprocess\n",
    "\n",
    "result = subprocess.run('bash -c \"source /etc/network_turbo && env | grep proxy\"', shell=True, capture_output=True,\n",
    "                        text=True)\n",
    "output = result.stdout\n",
    "for line in output.splitlines():\n",
    "    if '=' in line:\n",
    "        var, value = line.split('=', 1)\n",
    "        os.environ[var] = value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "100985a4-9420-4768-a4c8-44cba19ea057",
   "metadata": {},
   "outputs": [],
   "source": [
    "save_dir = '/root/autodl-tmp/models'\n",
    "save_name = \"google/gemma-2-9b-mingchao-ft\" \n",
    "out_dir = os.path.join(save_dir, save_name)\n",
    "\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
    "from peft import PeftModel, PeftConfig\n",
    "peft_config = PeftConfig.from_pretrained(out_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fcde458a-6b4c-4707-8c4f-3354d1444bfe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'google/gemma-2-9b'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_id = peft_config.base_model_name_or_path\n",
    "model_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6569e21-c59f-43e9-8fe1-833bee8ed0e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16\n",
    ")\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(model_id, quantization_config=bnb_config, device_map={\"\":0})\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id, add_eos_token=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2158bcf8-28ac-4e28-8523-2c28d2dd57b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_completion(query: str, model, tokenizer) -> str:\n",
    "  device = \"cuda:0\"\n",
    "\n",
    "  prompt_template = \"\"\"\n",
    "  ### 问题: {query}\\n###回答:\n",
    "  \"\"\"\n",
    "  prompt = prompt_template.format(query=query)\n",
    "\n",
    "  encodeds = tokenizer(prompt, return_tensors=\"pt\", add_special_tokens=True)\n",
    "\n",
    "  model_inputs = encodeds.to(device)\n",
    "\n",
    "\n",
    "  generated_ids = model.generate(**model_inputs, max_new_tokens=128, do_sample=True, pad_token_id=tokenizer.eos_token_id)\n",
    "  decoded = tokenizer.decode(generated_ids[0], skip_special_tokens=True)\n",
    "  return (decoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e54d617-e572-4ad6-8b98-0ee4c60e6771",
   "metadata": {},
   "outputs": [],
   "source": [
    "result = get_completion(query=\"《鸣潮》是什么呀？\", model=model, tokenizer=tokenizer)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c9ce656-ec26-4859-b9e6-a3204992ba95",
   "metadata": {},
   "outputs": [],
   "source": [
    "result = get_completion(query=\"《鸣潮》的特色是什么？\", model=model, tokenizer=tokenizer)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79047855-95f7-46d0-a61d-91ab83dfe5e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "result = get_completion(query=\"今汐是谁？\", model=model, tokenizer=tokenizer)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46ac350b-ad2a-4c70-a202-2668fa7a286e",
   "metadata": {},
   "outputs": [],
   "source": [
    "result = get_completion(query=\"今汐是有什么技能？\", model=model, tokenizer=tokenizer)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4a9635d-d096-4964-bdf2-b8cda0f94b8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "result = get_completion(query=\"对于XP党来说，抽取今汐是否值得抽取？\", model=model, tokenizer=tokenizer)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b3a1eb4-7d0c-48ce-b3d7-2edb30a5d2d5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
